{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFcttdwcNtYNHyS88W7Y4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nucleosight/nlp/blob/main/Urdu_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch Corpus sources"
      ],
      "metadata": {
        "id": "cXjQK8ZhBzuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/zeerakahmed/makhzan'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb03gq5_By5y",
        "outputId": "5b99d462-31e3-4323-dc80-945e8e41cf83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'makhzan'...\n",
            "remote: Enumerating objects: 17388, done.\u001b[K\n",
            "remote: Counting objects: 100% (389/389), done.\u001b[K\n",
            "remote: Compressing objects: 100% (375/375), done.\u001b[K\n",
            "remote: Total 17388 (delta 26), reused 84 (delta 14), pack-reused 16999 (from 1)\u001b[K\n",
            "Receiving objects: 100% (17388/17388), 130.65 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (10286/10286), done.\n",
            "Updating files: 100% (6346/6346), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "bsV-ZG1mWzIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "QsJ_cDeLB4G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data (if you haven't already)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess the text: tokenize, remove punctuation and stopwords\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = ['اب', 'ابھی', 'اپنا', 'اپنے', 'اپنی', 'اٹھا', 'اس', 'اسے', 'اسی', 'اگر', 'ان', 'انہوں', 'انہی', 'انہیں', 'انھیں', 'او', 'اور', 'اے', 'ایسا', 'ایسے', 'ایسی', 'ایک', 'آ', 'آپ', 'آتا', 'آتے', 'آتی', 'آگے', 'آنا', 'آنے', 'آنی', 'آئے', 'آئی', 'آئیں', 'آیا', 'با', 'بڑا', 'بڑے', 'بڑی', 'بعد', 'بعض', 'بلکہ', 'بہت', 'بھی', 'بے', 'پاس', 'پر', 'پہلے', 'پھر', 'تا', 'تاکہ', 'تب', 'تجھ', 'تجھے', 'تک', 'تم', 'تمام', 'تمہارا', 'تمہارے', 'تمھارے', 'تمہاری', 'تمہیں', 'تمھیں', 'تھا', 'تھے', 'تھی', 'تھیں', 'تو', 'تیری', 'تیرے', 'جا', 'جاتا', 'جاتی', 'جاتے', 'جاتی', 'جانے', 'جانی', 'جاؤ', 'جائے', 'جائیں', 'جب', 'جس', 'جن', 'جنہوں', 'جنہیں', 'جو', 'جیسا', 'جیسے', 'جیسی', 'جیسوں', 'چاہیئے', 'چلا', 'چاہے', 'چونکہ', 'حالاں', 'حالانکہ', 'دو', 'دونوں', 'دوں', 'دے', 'دی', 'دیا', 'دیں', 'دیے', 'دیتا', 'دیتے', 'دیتی', 'دینا', 'دینے', 'دینی', 'دیئے', 'ڈالا', 'ڈالنا', 'ڈالنے', 'ڈالنی', 'ڈالے', 'ڈالی', 'ذرا', 'رکھا', 'رکھتا', 'رکھتے', 'رکھتی', 'رکھنا', 'رکھنے', 'رکھنی', 'رکھے', 'رکھی', 'رہ', 'رہا', 'رہتا', 'رہتے', 'رہتی', 'رہنا', 'رہنے', 'رہنی', 'رہو', 'رہے', 'رہی', 'رہیں', 'زیادہ', 'سا', 'سامنے', 'سب', 'سکتا', 'سو', 'سے', 'سی', 'شاید', 'صرف', 'طرح', 'طرف', 'عین', 'کا', 'کبھی', 'کچھ', 'کہہ', 'کر', 'کرتا', 'کرتے', 'کرتی', 'کرنا', 'کرنے', 'کرو', 'کروں', 'کرے', 'کریں', 'کس', 'کسے', 'کسی', 'کہ', 'کہا', 'کہے', 'کو', 'کون', 'کوئی', 'کے', 'کی', 'کیا', 'کیسے', 'کیوں', 'کیونکہ', 'کیے', 'کئے', 'گا', 'گویا', 'گے', 'گی', 'گیا', 'گئے', 'گئی', 'لا', 'لاتا', 'لاتے', 'لاتی', 'لانا', 'لانے', 'لانی', 'لایا', 'لائے', 'لائی', 'لگا', 'لگے', 'لگی', 'لگیں', 'لو', 'لے', 'لی', 'لیا', 'لیتا', 'لیتے', 'لیتی', 'لیکن', 'لیں', 'لیے', 'لئے', 'مجھ', 'مجھے', 'مگر', 'میرا', 'میرے', 'میری', 'میں', 'نا', 'نہ', 'نہایت', 'نہیں', 'نے', 'ہاں', 'ہر', 'ہم', 'ہمارا', 'ہمارے', 'ہماری', 'ہو', 'ہوا', 'ہوتا', 'ہوتے', 'ہوتی', 'ہوتیں', 'ہوں', 'ہونا', 'ہونگے', 'ہونے', 'ہونی', 'ہوئے', 'ہوئی', 'ہوئیں', 'ہے', 'ہی', 'ہیں', 'و', 'والا', 'والوں', 'والے', 'والی', 'وہ', 'وہاں', 'وہی', 'وہیں', 'یا', 'یعنی', 'یہ', 'یہاں', 'یہی', 'یہیں']\n",
        "\n",
        "    #set(stopwords.words('urdu'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to read a corpus file and preprocess the text\n",
        "def read_corpus(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgFy2jnAmltc",
        "outputId": "b9d8ff59-139a-450b-adbd-796b158cb9aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Text Corpus"
      ],
      "metadata": {
        "id": "1bqBjAnotHfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract urdu text from  every xml file in the directory\n",
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "text_directory_path = '/content/makhzan/text'\n",
        "texts = []\n",
        "def extract_text_from_xml(file_path):\n",
        "  \"\"\"Extracts text from all elements in an XML file except those with the 'annotation' tag.\"\"\"\n",
        "  tree = ET.parse(file_path)\n",
        "  root = tree.getroot()\n",
        "  text = \"\"\n",
        "  for elem in root.iter():\n",
        "    if elem.tag  in [\"title\", \"p\"]:\n",
        "      text += elem.text or \"\"\n",
        "  return text\n",
        "\n",
        "for filename in os.listdir(text_directory_path):\n",
        "    if filename.endswith(\".xml\"):\n",
        "        file_path = os.path.join(text_directory_path, filename)\n",
        "        texts.append(extract_text_from_xml(file_path))"
      ],
      "metadata": {
        "id": "iR3iWhnptHIU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save extracted text into a file\n",
        "with open('/content/processed_makhzan_text.txt', 'w',\n",
        "          encoding='utf-8') as f:\n",
        "  for text in texts:\n",
        "    f.write(text + '\\n')\n"
      ],
      "metadata": {
        "id": "vhKhBUDSwWX0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the corpus\n",
        "corpus_file = '/content/processed_makhzan_text.txt'  # Specify your corpus file here\n",
        "corpus = [read_corpus(corpus_file)]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=corpus, vector_size=50, window=5, min_count=4, workers=8)\n",
        "\n",
        "# Save the model\n",
        "model.save('word2vec.model')\n"
      ],
      "metadata": {
        "id": "jNhBphwom4jy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage: Find similar words to a given word\n",
        "word = 'زمین'  # Replace with any word of your choice\n",
        "if word in model.wv:\n",
        "    similar_words = model.wv.most_similar(word)\n",
        "    print(f\"Words similar to '{word}':\")\n",
        "    for sim_word, score in similar_words:\n",
        "        print(f\"{sim_word}: {score}\")\n",
        "else:\n",
        "    print(f\"'{word}' not in vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsjaGZFHBYi8",
        "outputId": "cc4c97e0-c488-4635-c2b8-6e3a5105b32b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'زمین':\n",
            "مہان: 0.5318350195884705\n",
            "وی: 0.5101794004440308\n",
            "چمچے: 0.501324474811554\n",
            "۔یوں: 0.4957681894302368\n",
            "سفرسے: 0.4939730167388916\n",
            "کالا،: 0.4938831329345703\n",
            "لاوا: 0.49330034852027893\n",
            "بتاریخ: 0.4813132882118225\n",
            "مہاجنوں: 0.478219211101532\n",
            "۔سب: 0.4693716764450073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8vexNPZUfO3",
        "outputId": "751ae2da-ccdb-446e-ba00-47e8e23dca28"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211130"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}